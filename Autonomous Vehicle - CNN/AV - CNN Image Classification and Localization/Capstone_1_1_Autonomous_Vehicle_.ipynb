{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8v9iwZGan1s"
      },
      "source": [
        "## **Problem Statement 1:**\n",
        "Autonomous vehicles (AV) and intelligent transport systems (ITS) are the future of road transport. Automatic detection of vehicles on the road in real-time helps AV technology and makes ITS more intelligent in terms of vehicle tracking, vehicle counting, and road incident response.\n",
        "\n",
        "## **Objective 1:**\n",
        "As the first part of this project, you need to develop an AI model using a deep learning framework that predicts the type of vehicle present in an image as  well as localizes the vehicle by rectangular bounding box."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3JorMbhKc8Wo"
      },
      "source": [
        "1. Create a parent folder for custom model training and child folders to store data\n",
        "2. Prepare the dataset for model training and keep the following points in mind while\n",
        "preparing it\n",
        "â€¢ This dataset contains many images, and depending on the compute power of the VM, it\n",
        "might take a very long time to unzip this huge amount of data.\n",
        "3. Create an CNN architecture for object detection of your choice to train an object detection\n",
        "model. Please note that algorithm or architecture selection is a very important aspect of ML\n",
        "model training, and you must pick the one that works the best for your dataset.\n",
        "4. Evaluate the model and check the test results\n",
        "5. Run inferences on sample images and see if vehicles are detected accurately"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HgobT7TYne7t"
      },
      "source": [
        "## **Step 1: Create a Class to Prepare Dataset**\n",
        "\n",
        "With this structure:\n",
        "\n",
        "- You can automatically feed batches of images + labels + bounding boxes into your model\n",
        "\n",
        "- You can apply augmentations and transformations on the fly\n",
        "\n",
        "- It allows clean separation of data logic from training logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5pD4lnrHL4G"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import os\n",
        "import torch\n",
        "\n",
        "\n",
        "class VehicleDataset(Dataset):\n",
        "    def __init__(self, df, image_dir, transform=None, class_to_idx=None):\n",
        "        self.df = df                            # DataFrame with image names, labels, and bounding boxes\n",
        "        self.image_dir = image_dir              # Directory where image files are stored\n",
        "        self.transform = transform              # Any image transforms (resize, tensor conversion, etc.)\n",
        "        self.class_to_idx = class_to_idx        # Mapping from class name to integer (e.g., 'car': 0)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)                     # Total number of samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.image_dir, row['image_id']) #Loads the image file from disk\n",
        "        image = Image.open(img_path).convert(\"RGB\")    # Load and convert to RGB\n",
        "\n",
        "        label = self.class_to_idx[row['class']]        # Encode class label to an integer\n",
        "        bbox = torch.tensor([row['x_min'], row['y_min'], row['x_max'], row['y_max']], dtype=torch.float32) #Converts bounding box into a tensor\n",
        "\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)              # Apply transformations (resize, normalize, etc.)\n",
        "\n",
        "        return image, label, bbox                      # Return one sample\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMtFI7GYUQFx"
      },
      "source": [
        "## **Step 2: Clean CSV and Filter Missing Files**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OWOFe1-dUSd_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load labels\n",
        "column_names = ['image_id', 'class', 'x_min', 'y_min', 'x_max', 'y_max']\n",
        "df = pd.read_csv('/content/drive/MyDrive/AV-ITS Capstone Project /labels.csv',header=None,names=column_names)\n",
        "\n",
        "# Folder where your images are stored\n",
        "image_dir = '/content/drive/MyDrive/AV-ITS Capstone Project /Images'\n",
        "\n",
        "#The image_id in labels.csv is not padded with 0s. So to not cause errors between Images filenames and labels image_id we are padding to 8 digits\n",
        "df['image_id'] = df['image_id'].astype(str).str.zfill(8) + \".jpg\"\n",
        "\n",
        "# There might be rows in labels files for which there are no images in Image folder. So Only keep rows where image file exists\n",
        "df = df[df['image_id'].apply(lambda x: os.path.exists(os.path.join(image_dir, x)))]\n",
        "\n",
        "# Encode class labels\n",
        "class_names = df['class'].unique()\n",
        "class_to_idx = {cls: i for i, cls in enumerate(sorted(class_names))}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UIxkMlTbD__",
        "outputId": "7a006219-4d9a-467c-d44a-10ee962426b0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Number of rows in df: 17967\n",
            "Sample images in folder:\n",
            "['00004646.jpg', '00004645.jpg', '00004647.jpg', '00004620.jpg', '00004614.jpg']\n",
            "Sample filenames in df:\n",
            "0    00000000.jpg\n",
            "1    00000000.jpg\n",
            "2    00000000.jpg\n",
            "3    00000000.jpg\n",
            "4    00000000.jpg\n",
            "5    00000001.jpg\n",
            "6    00000001.jpg\n",
            "7    00000001.jpg\n",
            "8    00000001.jpg\n",
            "9    00000001.jpg\n",
            "Name: image_id, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of rows in df:\", len(df))\n",
        "import os\n",
        "\n",
        "print(\"Sample images in folder:\")\n",
        "print(os.listdir(image_dir)[:5])\n",
        "\n",
        "print(\"Sample filenames in df:\")\n",
        "print(df['image_id'].head(10))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jyoVYgUXV9k2"
      },
      "source": [
        "## **Step 3: Define CNN Model (Dual Head)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okZtGPTqXIBI"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ObjectClassifierAndLocalizer(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        # Shared CNN backbone\n",
        "        self.backbone = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, 3, stride=1, padding=1),  # Conv layer\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),  # Downsample\n",
        "\n",
        "            nn.Conv2d(16, 32, 3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "\n",
        "            nn.Conv2d(32, 64, 3, stride=1, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(2),\n",
        "        )\n",
        "\n",
        "        # ðŸ”¹ Flatten and shared dense layer\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(64 * 28 * 28, 512)  # assuming input images are 224x224\n",
        "\n",
        "        # ðŸ”¹ Output heads\n",
        "        self.class_head = nn.Linear(512, num_classes)  # classification output\n",
        "        self.bbox_head = nn.Linear(512, 4)             # bounding box output\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "\n",
        "        # Two parallel outputs\n",
        "        class_output = self.class_head(x)     # class logits (e.g., [0.2, 1.5, -0.6, ...])\n",
        "        bbox_output = self.bbox_head(x)       # 4 values: [x_min, y_min, x_max, y_max]\n",
        "\n",
        "        return class_output, bbox_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlSqrS9AIwim"
      },
      "source": [
        "## **Step 3 : Using Pre-trained model - ResNet-Based Dual Head Model (Classification + Localization) for faster result**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9OiWGKejInFj"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResNetClassifierLocalizer(nn.Module):\n",
        "    def __init__(self, num_classes):\n",
        "        super().__init__()\n",
        "\n",
        "        # Load pretrained ResNet18\n",
        "        resnet = models.resnet18(pretrained=True)\n",
        "\n",
        "        # Remove final fully connected layer (fc) to use as feature extractor\n",
        "        self.backbone = nn.Sequential(*list(resnet.children())[:-1])  # output: (batch, 512, 1, 1)\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(512, 256)\n",
        "\n",
        "        # Classification head\n",
        "        self.class_head = nn.Linear(256, num_classes)\n",
        "\n",
        "        # Bounding box head (x_min, y_min, x_max, y_max)\n",
        "        self.bbox_head = nn.Linear(256, 4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.backbone(x)             # (B, 512, 1, 1)\n",
        "        x = self.flatten(x)              # (B, 512)\n",
        "        x = F.relu(self.fc1(x))          # (B, 256)\n",
        "\n",
        "        class_output = self.class_head(x)\n",
        "        bbox_output = self.bbox_head(x)\n",
        "\n",
        "        return class_output, bbox_output\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nPsv02GadGo-"
      },
      "source": [
        "## **Step 4: Transform, Dataloader, and Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whtKRAoRZKdT"
      },
      "outputs": [],
      "source": [
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    #transforms.Resize((128,128)), #Reduced the size of the image further to check if the model reduce its training time\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "# Create Dataset and DataLoader\n",
        "dataset = VehicleDataset(df, image_dir=image_dir, transform=transform, class_to_idx=class_to_idx)\n",
        "\n",
        "# Split train/test\n",
        "from sklearn.model_selection import train_test_split\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, stratify=df['class'])\n",
        "\n",
        "train_dataset = VehicleDataset(train_df, image_dir, transform, class_to_idx)\n",
        "test_dataset = VehicleDataset(test_df, image_dir, transform, class_to_idx)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GEt-UHSDdKXf"
      },
      "source": [
        "## **Step 5: Train the Model (loss = classification + localization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFRBqQbaZtoK",
        "outputId": "0aa05f35-1000-42e7-9179-6bac2f7b0436"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 172MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1: Loss = 24373996.2422\n",
            "Epoch 2: Loss = 8283502.4795\n",
            "Epoch 3: Loss = 7955033.1855\n"
          ]
        }
      ],
      "source": [
        "#model = ObjectClassifierAndLocalizer(num_classes=len(class_names))\n",
        "model = ResNetClassifierLocalizer(num_classes=len(class_names))\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "cls_loss_fn = nn.CrossEntropyLoss()\n",
        "bbox_loss_fn = nn.MSELoss()\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(3):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, labels, bboxes in train_loader:\n",
        "        preds_cls, preds_bbox = model(images)\n",
        "\n",
        "        loss_cls = cls_loss_fn(preds_cls, labels)\n",
        "        loss_bbox = bbox_loss_fn(preds_bbox, bboxes)\n",
        "\n",
        "        loss = loss_cls + loss_bbox\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}: Loss = {total_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh4JfDNF0fYY"
      },
      "source": [
        "## **Step 6: Put Model in Evaluation Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Ydfvv4XdNWS"
      },
      "outputs": [],
      "source": [
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSLjKESA0tLM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "test_loss = 0\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels, bboxes in test_loader:\n",
        "        preds_cls, preds_bbox = model(images)\n",
        "\n",
        "        # Classification accuracy\n",
        "        _, predicted = torch.max(preds_cls, 1)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "        # Optional: localization loss\n",
        "        loss_cls = cls_loss_fn(preds_cls, labels)\n",
        "        loss_bbox = bbox_loss_fn(preds_bbox, bboxes)\n",
        "        test_loss += (loss_cls + loss_bbox).item()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4SY2Qe8E04Is"
      },
      "outputs": [],
      "source": [
        "print(f\"Test Accuracy (Classification): {100 * correct / total:.2f}%\")\n",
        "print(f\"Test Loss (Classification + BBox): {test_loss:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}